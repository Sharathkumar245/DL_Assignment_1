# DL_Assignment_1

I have designed a neural network using google colab and the final code is in the train.py file that we have updated here
Basically I have created the functions for each of the optimizer function.
We have created the function for each of the calculations for the training loss and accuracy and validation loss and accuracy
Here we use two dataset like mnist and fashion_mnist, from here we will get the datasets like images and their corresponding labels and we will get the test images and test labels to this will be helpful while we are dealing with the 'Confusion matrix'

Created a function main_function through which we will call the remaining function like intitalize_w_and_b() and optimization_function() which will be helpful to call the function according to the parameters that we passed through the wandb. Wandb is a great tool that will be helpful to plot the plots. 
We will import the packages that we need to import into the colab file or if we are working offline then we need to download and install the packages that are required like wandb, keras,numpy and some other..

Weight initialisations: 
here we used random and xavier functions. Here 'random' will be used to intialise the w and b's(parameters) randomly and provide provide this as an input parameter to the optimiser function, whereas xavier is a used to intialize the weights randomly but where as coming to biases(b's) we assign those to all zeroes. These are called as the input parmeters to the optimisation_function().

optimisation_function:
This function is a kind of  function in means of importance on what we are doing. From this function we will pass which optimiser we want to call and with what variant of gradient descrent we want to work with.
Some of the optimisers that we used here is stocahstic,momentum,nesterov,rmsprop,adam,nadam there are some other too but we will do these optimisers in this code.

stochastic gradient descent is a kind of varient that which will update it's weight after every calculation of gradients i.e, back propagations.
batch size is usually used to update the weights and biases after some points caculation for gradients.]

forward_propagation:
here we will pass w's and b's as the parameters and to the functions and this will helps to calculate the activations,pre-activations that we used in the calcation of the backward_propagation. 

back_propagation:
here we usually back propagate through the whole network and try to update the weights and biases to predict the possible values that needed to be updated to the weights and biases that we are trying to calculate.

calculation_of_loss_and_accuracy:
Here we usually try to predict the accuracy and loss from the weights that we generated by back propagating.

confusion matrix:
This is a kind of matrix that which will help us to find the labels that which our models will provide us the information that how many of the test points that our model can be able to predict correctly and this will give us an idea about the model how good it is predicting the values.

Here we will run the DL_assignment.ipynd
